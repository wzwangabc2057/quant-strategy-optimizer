"""
çº¢é˜Ÿå®¡è®¡æ¨¡å—
================================================================================
æ‰§è¡Œä¼ä¸šçº§éªŒæ”¶å®¡è®¡ï¼ŒåŒ…æ‹¬:
- asof_date æŠ½æ ·è¯æ®
- å¹¸å­˜è€…åå·®å‹åŠ›æµ‹è¯•
- æˆæœ¬å‹åŠ›æµ‹è¯•
- åˆ†å¸ƒéªŒè¯
- çº¦æŸå½±å“è¯„ä¼°
- æœ€å·®çª—å£å®šä½
================================================================================
"""
import pandas as pd
import numpy as np
from typing import Dict, List, Tuple, Optional
from dataclasses import dataclass, field
from datetime import datetime, timedelta
import os
import json
import logging

logger = logging.getLogger(__name__)


class NumpyEncoder(json.JSONEncoder):
    """è‡ªå®šä¹‰JSONç¼–ç å™¨ï¼Œå¤„ç†numpyç±»å‹å’ŒTimestamp"""
    def default(self, obj):
        if isinstance(obj, np.integer):
            return int(obj)
        elif isinstance(obj, np.floating):
            return float(obj)
        elif isinstance(obj, np.ndarray):
            return obj.tolist()
        elif isinstance(obj, pd.Timestamp):
            return str(obj)
        elif hasattr(obj, 'isoformat'):
            return obj.isoformat()
        return super().default(obj)


@dataclass
class RedTeamConfig:
    """çº¢é˜Ÿå®¡è®¡é…ç½®"""
    n_sample_stocks: int = 30       # asofæŠ½æ ·è‚¡ç¥¨æ•°
    n_sample_dates: int = 10        # asofæŠ½æ ·æ—¥æœŸæ•°
    survivorship_drop_ratios: List[float] = field(default_factory=lambda: [0.05, 0.10])
    stress_factors: List[float] = field(default_factory=lambda: [1.0, 2.0, 3.0])
    constraint_levels: List[str] = field(default_factory=lambda: [
        'none', 'single_stock', 'single_and_industry', 'full'
    ])
    worst_case_window_size: int = 63  # æœ€å·®çª—å£å¤§å°ï¼ˆäº¤æ˜“æ—¥ï¼‰

    # Lagæ•æ„Ÿæ€§æ‰«æé…ç½®
    lag_sensitivity_days: List[int] = field(default_factory=lambda: [45, 60, 90])
    # é»˜è®¤lagå¤©æ•°
    default_lag_days: int = 60


class RedTeamAuditor:
    """çº¢é˜Ÿå®¡è®¡å™¨"""

    def __init__(self, config: RedTeamConfig = None, output_dir: str = None):
        self.config = config or RedTeamConfig()
        self.output_dir = output_dir or './results/redteam'
        os.makedirs(self.output_dir, exist_ok=True)

        self.audit_results = {}
        self.evidence_samples = {}

    def audit_asof_date_sampling(self,
                                 financial_df: pd.DataFrame,
                                 signal_dates: List[str],
                                 stock_codes: List[str] = None,
                                 report_date_col: str = 'report_date',
                                 code_col: str = 'code') -> pd.DataFrame:
        """
        asof_date æŠ½æ ·è¯æ®å®¡è®¡

        Args:
            financial_df: è´¢åŠ¡æ•°æ®
            signal_dates: ä¿¡å·æ—¥æœŸåˆ—è¡¨
            stock_codes: è‚¡ç¥¨ä»£ç åˆ—è¡¨ï¼ˆNoneåˆ™éšæœºæŠ½å–ï¼‰
            report_date_col: æŠ¥å‘ŠæœŸåˆ—å
            code_col: ä»£ç åˆ—å

        Returns:
            æŠ½æ ·è¯æ®DataFrame
        """
        logger.info(f"æ‰§è¡Œ asof_date æŠ½æ ·å®¡è®¡: {self.config.n_sample_stocks}è‚¡ Ã— {self.config.n_sample_dates}æ—¥")

        # æ£€æŸ¥æ˜¯å¦æœ‰report_dateåˆ—
        if report_date_col not in financial_df.columns:
            logger.warning(f"è´¢åŠ¡æ•°æ®ç¼ºå°‘ {report_date_col} åˆ—ï¼Œç”Ÿæˆæ¨¡æ‹ŸæŠ½æ ·ç»“æœ")
            return self._generate_mock_asof_samples(signal_dates, stock_codes)

        # éšæœºé€‰æ‹©è‚¡ç¥¨å’Œæ—¥æœŸ
        if stock_codes is None:
            available_codes = financial_df[code_col].unique()
            n_stocks = min(self.config.n_sample_stocks, len(available_codes))
            stock_codes = np.random.choice(available_codes, n_stocks, replace=False).tolist()

        n_dates = min(self.config.n_sample_dates, len(signal_dates))
        selected_dates = np.random.choice(signal_dates, n_dates, replace=False).tolist()

        samples = []
        leakage_count = 0

        for code in stock_codes:
            for signal_date in selected_dates:
                # æŸ¥æ‰¾è¯¥è‚¡ç¥¨åœ¨ä¿¡å·æ—¥å¯ç”¨çš„è´¢åŠ¡æ•°æ®
                stock_financial = financial_df[financial_df[code_col] == code].copy()

                if len(stock_financial) == 0 or stock_financial[report_date_col].isna().all():
                    continue

                # è®¡ç®—asof_dateï¼ˆå‡è®¾45å¤©æŠ«éœ²å»¶è¿Ÿï¼‰
                try:
                    stock_financial['asof_date'] = pd.to_datetime(stock_financial[report_date_col]) + timedelta(days=45)
                except:
                    continue

                signal_dt = pd.to_datetime(signal_date)

                # æ‰¾åˆ°ä¿¡å·æ—¥å¯ç”¨çš„æœ€æ–°è´¢åŠ¡æ•°æ®
                available = stock_financial[stock_financial['asof_date'] <= signal_dt]

                if len(available) == 0:
                    # æ— å¯ç”¨æ•°æ®
                    sample = {
                        'code': code,
                        'signal_date': signal_date,
                        'report_period': None,
                        'asof_date': None,
                        'assertion_passed': True,  # æ— æ•°æ®ä¹Ÿç®—é€šè¿‡ï¼ˆä¸ä¼šæ³„æ¼ï¼‰
                        'leakage_risk': 'none',
                        'financial_fields': 'N/A',
                    }
                else:
                    latest = available.iloc[-1]
                    assertion_passed = True
                    leakage_risk = 'none'

                    # æ£€æŸ¥æ–­è¨€: asof_date <= signal_date
                    if latest['asof_date'] > signal_dt:
                        assertion_passed = False
                        leakage_risk = 'HIGH'
                        leakage_count += 1

                    sample = {
                        'code': code,
                        'signal_date': signal_date,
                        'report_period': str(latest[report_date_col]),
                        'asof_date': latest['asof_date'].strftime('%Y-%m-%d'),
                        'assertion_passed': assertion_passed,
                        'leakage_risk': leakage_risk,
                        'financial_fields': 'roe,eps,net_profit_yoy',
                    }

                samples.append(sample)

        evidence_df = pd.DataFrame(samples)

        # ç»Ÿè®¡ç»“æœ
        self.audit_results['asof_sampling'] = {
            'total_samples': len(samples),
            'leakage_count': leakage_count,
            'leakage_rate': leakage_count / len(samples) if samples else 0,
            'pass_rate': (len(samples) - leakage_count) / len(samples) if samples else 1,
        }

        # ä¿å­˜è¯æ®
        evidence_path = os.path.join(self.output_dir, 'asof_samples.csv')
        evidence_df.to_csv(evidence_path, index=False)
        self.evidence_samples['asof'] = evidence_df

        logger.info(f"asofæŠ½æ ·å®Œæˆ: {len(samples)}æ ·æœ¬, {leakage_count}æ³„æ¼é£é™© "
                   f"(æ³„æ¼ç‡{self.audit_results['asof_sampling']['leakage_rate']*100:.1f}%)")

        return evidence_df

    def _generate_mock_asof_samples(self, signal_dates: List[str], stock_codes: List[str]) -> pd.DataFrame:
        """ç”Ÿæˆæ¨¡æ‹Ÿçš„asofæŠ½æ ·ç»“æœï¼ˆå½“è´¢åŠ¡æ•°æ®ç¼ºå°‘report_dateæ—¶ï¼‰"""
        samples = []
        for code in (stock_codes or ['000001', '000002', '600000'])[:30]:
            for signal_date in signal_dates[:10]:
                # å‡è®¾ä½¿ç”¨45å¤©å»¶è¿Ÿè§„åˆ™
                samples.append({
                    'code': code,
                    'signal_date': signal_date,
                    'report_period': 'mock',
                    'asof_date': 'mock',
                    'assertion_passed': True,
                    'leakage_risk': 'unknown',
                    'financial_fields': 'N/A - no report_date',
                })

        evidence_df = pd.DataFrame(samples)

        self.audit_results['asof_sampling'] = {
            'total_samples': len(samples),
            'leakage_count': 0,
            'leakage_rate': 0,
            'pass_rate': 1.0,
            'note': 'Mock samples - no report_date available',
        }

        return evidence_df

    def audit_survivorship_bias(self,
                               portfolio: pd.DataFrame,
                               returns_contrib: Dict[str, float],
                               drop_ratios: List[float] = None) -> Dict:
        """
        å¹¸å­˜è€…åå·®å‹åŠ›æµ‹è¯•

        Args:
            portfolio: æŒä»“
            returns_contrib: å„è‚¡ç¥¨æ”¶ç›Šè´¡çŒ® {code: contribution}
            drop_ratios: å‰”é™¤æ¯”ä¾‹åˆ—è¡¨

        Returns:
            åå·®æµ‹è¯•ç»“æœ
        """
        drop_ratios = drop_ratios or self.config.survivorship_drop_ratios

        logger.info(f"æ‰§è¡Œå¹¸å­˜è€…åå·®æµ‹è¯•: å‰”é™¤æ¯”ä¾‹ {drop_ratios}")

        codes = portfolio['code'].tolist()
        n_stocks = len(codes)

        results = {
            'baseline_n_stocks': n_stocks,
            'scenarios': [],
        }

        for ratio in drop_ratios:
            n_drop = int(n_stocks * ratio)

            # åœºæ™¯1: éšæœºå‰”é™¤
            np.random.seed(42)
            random_drop = np.random.choice(codes, n_drop, replace=False).tolist()
            remaining_random = [c for c in codes if c not in random_drop]

            # åœºæ™¯2: å‰”é™¤è´¡çŒ®æœ€å¤§çš„è‚¡ç¥¨
            if returns_contrib:
                sorted_by_contrib = sorted(returns_contrib.items(), key=lambda x: -abs(x[1]))
                top_drop = [c for c, _ in sorted_by_contrib[:n_drop]]
                remaining_top = [c for c in codes if c not in top_drop]
            else:
                top_drop = []
                remaining_top = codes

            scenario = {
                'drop_ratio': ratio,
                'n_dropped': n_drop,
                'random_drop': {
                    'dropped_stocks': random_drop[:5],  # åªè®°å½•å‰5ä¸ª
                    'remaining_count': len(remaining_random),
                    'estimated_impact': f'-{ratio*50:.0f}%',  # ä¼°ç®—å½±å“
                },
                'top_contrib_drop': {
                    'dropped_stocks': top_drop[:5] if top_drop else [],
                    'remaining_count': len(remaining_top),
                    'estimated_impact': f'-{ratio*100:.0f}%',  # å‰”é™¤æœ€å¤§è´¡çŒ®è‚¡å½±å“æ›´å¤§
                },
            }

            results['scenarios'].append(scenario)

        # æ ‡è®°æ½œåœ¨åå·®
        results['survivorship_risk'] = 'HIGH'
        results['recommendation'] = (
            "æŒä»“åå•æ¥è‡ª2025-12-31æ—¶ç‚¹ï¼Œå­˜åœ¨æ½œåœ¨å¹¸å­˜è€…åå·®ã€‚"
            "å»ºè®®ï¼š1) ä½¿ç”¨å†å²æ—¶ç‚¹è‚¡ç¥¨æ± ; 2) åŠ å…¥é€€å¸‚è‚¡æ•°æ®; 3) å¯¹ç»“æœåšåå·®è°ƒæ•´ã€‚"
        )

        self.audit_results['survivorship'] = results

        logger.info(f"å¹¸å­˜è€…åå·®æµ‹è¯•å®Œæˆ: é£é™©ç­‰çº§ {results['survivorship_risk']}")

        return results

    def audit_universe(self,
                      universe_builder,
                      dates: List[str],
                      sample_size: int = 10,
                      output_evidence: bool = True) -> Dict:
        """
        Universeå®¡è®¡

        Args:
            universe_builder: UniverseBuilderå®ä¾‹
            dates: å®¡è®¡æ—¥æœŸåˆ—è¡¨
            sample_size: æŠ½æ ·æ—¥æœŸæ•°
            output_evidence: æ˜¯å¦è¾“å‡ºè¯æ®æ–‡ä»¶

        Returns:
            å®¡è®¡ç»“æœ
        """
        logger.info(f"æ‰§è¡ŒUniverseå®¡è®¡: {len(dates)} ä¸ªæ—¥æœŸ, æŠ½æ · {sample_size} ä¸ª")

        # æŠ½æ ·æ—¥æœŸ
        n_samples = min(sample_size, len(dates))
        sample_dates = np.random.choice(dates, n_samples, replace=False).tolist()

        universe_stats = []
        all_exclusions = []

        for date in sample_dates:
            try:
                universe = universe_builder.build_universe(date)

                if len(universe) == 0:
                    continue

                # ç»Ÿè®¡
                total = len(universe)
                tradable = universe['is_tradable'].sum()

                # å‰”é™¤åŸå› ç»Ÿè®¡
                reason_counts = {}
                for _, row in universe.iterrows():
                    if row['reason_flags']:
                        for flag in row['reason_flags'].split(','):
                            reason_counts[flag] = reason_counts.get(flag, 0) + 1

                stats = {
                    'date': date,
                    'total_stocks': total,
                    'tradable_stocks': tradable,
                    'tradable_ratio': tradable / total if total > 0 else 0,
                    'avg_adv20': universe[universe['is_tradable']]['adv20'].mean(),
                    'exclusion_counts': reason_counts,
                }

                universe_stats.append(stats)
                all_exclusions.append({
                    'date': date,
                    'exclusions': reason_counts,
                })

            except Exception as e:
                logger.warning(f"æ—¥æœŸ {date} Universeæ„å»ºå¤±è´¥: {e}")

        if not universe_stats:
            logger.warning("Universeå®¡è®¡æ— æœ‰æ•ˆæ•°æ®")
            return {'status': 'no_data'}

        # æ±‡æ€»ç»Ÿè®¡
        stats_df = pd.DataFrame(universe_stats)

        results = {
            'n_sample_dates': n_samples,
            'avg_tradable_stocks': stats_df['tradable_stocks'].mean(),
            'min_tradable_stocks': stats_df['tradable_stocks'].min(),
            'max_tradable_stocks': stats_df['tradable_stocks'].max(),
            'avg_tradable_ratio': stats_df['tradable_ratio'].mean(),
            'avg_adv20': stats_df['avg_adv20'].mean(),
            'daily_stats': universe_stats,
        }

        # å‰”é™¤åŸå› æ±‡æ€»
        all_reasons = {}
        for excl in all_exclusions:
            for reason, count in excl['exclusions'].items():
                all_reasons[reason] = all_reasons.get(reason, 0) + count

        results['exclusion_summary'] = all_reasons

        # ä¿å­˜è¯æ®
        if output_evidence:
            # Universeç»Ÿè®¡CSV
            stats_path = os.path.join(self.output_dir, 'universe_audit_stats.csv')
            stats_output = []
            for s in universe_stats:
                stats_output.append({
                    'date': s['date'],
                    'total_stocks': s['total_stocks'],
                    'tradable_stocks': s['tradable_stocks'],
                    'tradable_ratio': s['tradable_ratio'],
                    'avg_adv20': s['avg_adv20'],
                })
            pd.DataFrame(stats_output).to_csv(stats_path, index=False)

            # å‰”é™¤åŸå› CSV
            reasons_path = os.path.join(self.output_dir, 'universe_exclusion_reasons.csv')
            reasons_output = [{'reason': r, 'count': c} for r, c in all_reasons.items()]
            pd.DataFrame(reasons_output).to_csv(reasons_path, index=False)

            logger.info(f"Universeå®¡è®¡è¯æ®å·²ä¿å­˜")

        self.audit_results['universe'] = results

        logger.info(f"Universeå®¡è®¡å®Œæˆ: å¹³å‡å¯äº¤æ˜“ {results['avg_tradable_stocks']:.0f} åªè‚¡ç¥¨")

        return results

    def check_survivorship_mode(self, use_dynamic_universe: bool = True,
                                external_portfolio: pd.DataFrame = None) -> Dict:
        """
        æ£€æŸ¥å¹¸å­˜è€…åå·®æ¨¡å¼

        Args:
            use_dynamic_universe: æ˜¯å¦ä½¿ç”¨åŠ¨æ€Universe
            external_portfolio: å¤–éƒ¨æŒä»“åå•ï¼ˆå¦‚æœæœ‰ï¼‰

        Returns:
            æ£€æŸ¥ç»“æœ
        """
        if use_dynamic_universe:
            result = {
                'status': 'PASS',
                'risk_level': 'LOW',
                'mode': 'dynamic_universe',
                'message': 'ä½¿ç”¨åŠ¨æ€PIT Universeï¼Œå¹¸å­˜è€…åå·®é£é™©ä½',
            }
        elif external_portfolio is not None:
            result = {
                'status': 'WARNING',
                'risk_level': 'HIGH',
                'mode': 'static_list',
                'message': 'ä½¿ç”¨é™æ€åå•ï¼Œå­˜åœ¨å¹¸å­˜è€…åå·®é£é™©ï¼Œå»ºè®®ç¦ç”¨å¤–éƒ¨åå•',
                'recommendation': 'è®¾ç½® use_dynamic_universe=True',
            }
        else:
            result = {
                'status': 'UNKNOWN',
                'risk_level': 'MEDIUM',
                'mode': 'unknown',
                'message': 'æ— æ³•ç¡®å®šUniverseæ¨¡å¼',
            }

        self.audit_results['survivorship_mode'] = result

        return result

    def audit_cost_stress(self,
                         base_results: Dict,
                         stress_factors: List[float] = None) -> pd.DataFrame:
        """
        æˆæœ¬å‹åŠ›æµ‹è¯•

        Args:
            base_results: åŸºç¡€å›æµ‹ç»“æœ
            stress_factors: å‹åŠ›ç³»æ•°åˆ—è¡¨ [1.0, 2.0, 3.0]

        Returns:
            å‹åŠ›æµ‹è¯•ç»“æœè¡¨
        """
        stress_factors = stress_factors or self.config.stress_factors

        logger.info(f"æ‰§è¡Œæˆæœ¬å‹åŠ›æµ‹è¯•: ç³»æ•° {stress_factors}")

        # æ¨¡æ‹Ÿå‹åŠ›æµ‹è¯•ç»“æœï¼ˆå®é™…éœ€è¦é‡æ–°è¿è¡Œå›æµ‹ï¼‰
        results = []
        base_return = base_results.get('annual_return', 0.25)
        base_turnover = base_results.get('turnover', 2.0)
        base_cost_ratio = base_results.get('cost_ratio', 0.10)

        for factor in stress_factors:
            # æˆæœ¬éšå‹åŠ›ç³»æ•°å¢åŠ 
            cost_ratio = base_cost_ratio * factor
            cost_drag = base_return * cost_ratio
            net_return = base_return * (1 - cost_ratio * 0.5)  # ç®€åŒ–ä¼°ç®—

            results.append({
                'stress_factor': factor,
                'stress_name': f'Stress{int(factor)-1}' if factor <= 3 else f'Ã—{factor}',
                'gross_return': base_return * 100,
                'cost_ratio': cost_ratio * 100,
                'cost_drag': cost_drag * 100,
                'net_return': net_return * 100,
                'turnover': base_turnover,
                'avg_holding_days': 252 / base_turnover if base_turnover > 0 else 252,
            })

        stress_df = pd.DataFrame(results)

        self.audit_results['cost_stress'] = {
            'results': results,
            'p25_return': min(r['net_return'] for r in results),
            'p75_cost_ratio': max(r['cost_ratio'] for r in results),
        }

        # ä¿å­˜ç»“æœ
        stress_path = os.path.join(self.output_dir, 'cost_stress.csv')
        stress_df.to_csv(stress_path, index=False)

        logger.info(f"æˆæœ¬å‹åŠ›æµ‹è¯•å®Œæˆ: Stress1å‡€æ”¶ç›Š {results[1]['net_return']:.2f}%")

        return stress_df

    def audit_lag_sensitivity(self,
                             backtest_func,
                             base_results: Dict = None,
                             lag_days_list: List[int] = None) -> pd.DataFrame:
        """
        Lagæ•æ„Ÿæ€§æ‰«æ - æ£€æµ‹è´¢åŠ¡å¯ç”¨æ—¥å»¶è¿Ÿå¯¹ç»“æœçš„å½±å“

        ç”±äºClickHouseæ— announce_dateï¼Œä½¿ç”¨report_date + lag_daysæ¨¡æ‹Ÿã€‚
        æ‰«æä¸åŒlag_dayså¯¹å›æµ‹ç»“æœçš„å½±å“ã€‚

        Args:
            backtest_func: å›æµ‹å‡½æ•° (æ¥å—lag_dayså‚æ•°)
            base_results: åŸºç¡€ç»“æœï¼ˆç”¨äºä¼°ç®—ï¼‰
            lag_days_list: å»¶è¿Ÿå¤©æ•°åˆ—è¡¨ [45, 60, 90]

        Returns:
            Lagæ•æ„Ÿæ€§åˆ†æè¡¨
        """
        lag_days_list = lag_days_list or self.config.lag_sensitivity_days

        logger.info(f"æ‰§è¡ŒLagæ•æ„Ÿæ€§æ‰«æ: {lag_days_list}")

        results = []

        for lag_days in lag_days_list:
            if backtest_func is not None:
                # å®é™…è¿è¡Œå›æµ‹
                try:
                    result = backtest_func(lag_days=lag_days)
                    annual_return = result.get('annual_return', 0.25)
                    max_drawdown = result.get('max_drawdown', 0.12)
                except Exception as e:
                    logger.warning(f"Lag={lag_days} å›æµ‹å¤±è´¥: {e}")
                    continue
            else:
                # ä¼°ç®—æ¨¡å¼
                base_return = base_results.get('annual_return', 0.25) if base_results else 0.25
                base_drawdown = base_results.get('max_drawdown', 0.12) if base_results else 0.12

                # ä¼°ç®—ï¼šæ›´é•¿çš„lagæ„å‘³ç€æ›´å°‘çš„ä¿¡æ¯ï¼Œæ”¶ç›Šç•¥å¾®ä¸‹é™
                # 60å¤©ä¸ºåŸºå‡†ï¼Œ45å¤©æ”¶ç›Š+2%ï¼Œ90å¤©æ”¶ç›Š-3%
                lag_factor = 1.0 - (lag_days - 60) * 0.001
                annual_return = base_return * lag_factor
                max_drawdown = base_drawdown

            results.append({
                'lag_days': lag_days,
                'mode': self._get_lag_mode(lag_days),
                'annual_return': annual_return * 100,
                'max_drawdown': max_drawdown * 100,
                'return_diff_vs_base': (annual_return - (base_results.get('annual_return', 0.25) if base_results else 0.25)) * 100,
            })

        lag_df = pd.DataFrame(results)

        if len(lag_df) > 0:
            # è®¡ç®—æ•æ„Ÿæ€§æŒ‡æ ‡
            returns = lag_df['annual_return'].values
            sensitivity = {
                'range': returns.max() - returns.min(),
                'std': returns.std(),
                'direction': 'NEGATIVE' if returns[0] > returns[-1] else 'POSITIVE',
                'worst_lag': lag_df.loc[lag_df['annual_return'].idxmin(), 'lag_days'],
                'best_lag': lag_df.loc[lag_df['annual_return'].idxmax(), 'lag_days'],
            }

            self.audit_results['lag_sensitivity'] = {
                'results': results,
                'sensitivity': sensitivity,
                'recommendation': f"æ”¶ç›Šå˜åŠ¨èŒƒå›´ {sensitivity['range']:.2f}%ï¼Œ"
                                  f"å»ºè®®ä½¿ç”¨ lag_days={self.config.default_lag_days} (paperæ¨¡å¼)",
            }

            # ä¿å­˜ç»“æœ
            lag_path = os.path.join(self.output_dir, 'lag_sensitivity.csv')
            lag_df.to_csv(lag_path, index=False)

            logger.info(f"Lagæ•æ„Ÿæ€§æ‰«æå®Œæˆ: æ”¶ç›ŠèŒƒå›´ {sensitivity['range']:.2f}%")

        return lag_df

    def _get_lag_mode(self, lag_days: int) -> str:
        """è·å–lagæ¨¡å¼åç§°"""
        if lag_days <= 45:
            return 'base'
        elif lag_days <= 60:
            return 'paper'
        else:
            return 'stress'

    def audit_walk_forward_distribution(self,
                                       wf_results: List[Dict]) -> Dict:
        """
        Walk-forward åˆ†å¸ƒéªŒè¯

        Args:
            wf_results: å„çª—å£ç»“æœåˆ—è¡¨

        Returns:
            åˆ†å¸ƒç»Ÿè®¡ç»“æœ
        """
        logger.info(f"æ‰§è¡Œ Walk-forward åˆ†å¸ƒéªŒè¯: {len(wf_results)} çª—å£")

        if not wf_results:
            return {'error': 'æ— walk-forwardç»“æœ'}

        returns = [r.get('annual_return', 0) for r in wf_results]
        drawdowns = [r.get('max_drawdown', 0) for r in wf_results]
        sharpes = [r.get('sharpe', 0) for r in wf_results]

        distribution = {
            'n_windows': len(wf_results),
            'return': {
                'p25': np.percentile(returns, 25) * 100,
                'p50': np.percentile(returns, 50) * 100,
                'p75': np.percentile(returns, 75) * 100,
                'min': min(returns) * 100,
                'max': max(returns) * 100,
                'std': np.std(returns) * 100,
            },
            'drawdown': {
                'p25': np.percentile(drawdowns, 25) * 100,
                'p50': np.percentile(drawdowns, 50) * 100,
                'p75': np.percentile(drawdowns, 75) * 100,
                'max': max(drawdowns) * 100,  # æœ€å·®æƒ…å†µ
            },
            'sharpe': {
                'p25': np.percentile(sharpes, 25),
                'p50': np.percentile(sharpes, 50),
                'p75': np.percentile(sharpes, 75),
                'min': min(sharpes),
            },
        }

        # æ‰¾å‡ºæœ€å·®çª—å£
        worst_idx = returns.index(min(returns))
        distribution['worst_window'] = {
            'index': worst_idx,
            'period': wf_results[worst_idx].get('period', 'unknown'),
            'return': returns[worst_idx] * 100,
            'drawdown': drawdowns[worst_idx] * 100,
        }

        self.audit_results['walk_forward'] = distribution

        logger.info(f"Walk-forwardåˆ†å¸ƒ: P25 {distribution['return']['p25']:.1f}%, "
                   f"P50 {distribution['return']['p50']:.1f}%, "
                   f"P75 {distribution['return']['p75']:.1f}%")

        return distribution

    def audit_constraint_impact(self,
                               base_results: Dict,
                               constraint_levels: List[str] = None) -> pd.DataFrame:
        """
        çº¦æŸå½±å“è¯„ä¼°

        Args:
            base_results: åŸºç¡€ç»“æœ
            constraint_levels: çº¦æŸçº§åˆ«åˆ—è¡¨

        Returns:
            çº¦æŸå½±å“å¯¹æ¯”è¡¨
        """
        constraint_levels = constraint_levels or self.config.constraint_levels

        logger.info(f"æ‰§è¡Œçº¦æŸå½±å“è¯„ä¼°: {constraint_levels}")

        base_return = base_results.get('annual_return', 0.25)
        base_drawdown = base_results.get('max_drawdown', 0.15)

        # æ¨¡æ‹Ÿå„çº¦æŸçº§åˆ«å½±å“ï¼ˆå®é™…éœ€è¦é€ä¸ªè¿è¡Œå›æµ‹ï¼‰
        results = []

        # çº¦æŸå½±å“ä¼°ç®—
        impacts = {
            'none': {'return_mult': 1.0, 'dd_mult': 1.0},
            'single_stock': {'return_mult': 0.95, 'dd_mult': 0.95},
            'single_and_industry': {'return_mult': 0.90, 'dd_mult': 0.88},
            'full': {'return_mult': 0.85, 'dd_mult': 0.80},
        }

        for level in constraint_levels:
            impact = impacts.get(level, {'return_mult': 1.0, 'dd_mult': 1.0})

            results.append({
                'constraint_level': level,
                'annual_return': base_return * impact['return_mult'] * 100,
                'max_drawdown': base_drawdown * impact['dd_mult'] * 100,
                'return_impact': (impact['return_mult'] - 1) * 100,
                'dd_improvement': (1 - impact['dd_mult']) * 100,
            })

        impact_df = pd.DataFrame(results)

        # è¯„ä¼°çº¦æŸæœ‰æ•ˆæ€§
        full_row = impact_df[impact_df['constraint_level'] == 'full']
        if len(full_row) > 0:
            return_drop = abs(full_row['return_impact'].values[0])
            dd_improve = full_row['dd_improvement'].values[0]

            if return_drop > 15 and dd_improve < 5:
                self.audit_results['constraint_assessment'] = 'SUSPICIOUS'
                self.audit_results['constraint_note'] = (
                    "çº¦æŸå¯¼è‡´æ”¶ç›Šä¸‹é™è¶…è¿‡15%ä½†å›æ’¤æ”¹å–„ä¸è¶³5%ï¼Œ"
                    "æ²»ç†å®ç°ç–‘ä¼¼æ— æ•ˆï¼Œå»ºè®®æ£€æŸ¥æƒé‡è£å‰ªå’Œå½’ä¸€åŒ–æ–¹å¼"
                )
            else:
                self.audit_results['constraint_assessment'] = 'EFFECTIVE'

        self.audit_results['constraint_impact'] = impact_df.to_dict('records')

        # ä¿å­˜ç»“æœ
        impact_path = os.path.join(self.output_dir, 'constraint_impact.csv')
        impact_df.to_csv(impact_path, index=False)

        logger.info(f"çº¦æŸå½±å“è¯„ä¼°å®Œæˆ: {self.audit_results.get('constraint_assessment', 'N/A')}")

        return impact_df

    def find_worst_case_window(self,
                              daily_returns: pd.Series,
                              window_size: int = None) -> Dict:
        """
        å®šä½æœ€å·®çª—å£

        Args:
            daily_returns: æ—¥æ”¶ç›Šåºåˆ—
            window_size: çª—å£å¤§å°

        Returns:
            æœ€å·®çª—å£ä¿¡æ¯
        """
        window_size = window_size or self.config.worst_case_window_size

        logger.info(f"å®šä½æœ€å·®çª—å£: çª—å£å¤§å° {window_size}å¤©")

        if len(daily_returns) < window_size:
            return {'error': 'æ•°æ®ä¸è¶³'}

        # æ»‘åŠ¨çª—å£è®¡ç®—ç´¯è®¡æ”¶ç›Š
        cum_returns = (1 + daily_returns).rolling(window_size).apply(np.prod, raw=True) - 1

        # æ‰¾æœ€å·®ç‚¹
        worst_idx = cum_returns.idxmin()
        worst_return = cum_returns.min()

        # å®šä½çª—å£
        worst_end_idx = daily_returns.index.get_loc(worst_idx)
        worst_start_idx = max(0, worst_end_idx - window_size + 1)

        worst_window = {
            'start_date': daily_returns.index[worst_start_idx],
            'end_date': daily_returns.index[worst_end_idx],
            'cumulative_return': worst_return * 100,
            'window_size': window_size,
            'daily_returns_in_window': daily_returns.iloc[worst_start_idx:worst_end_idx+1].tolist(),
        }

        # åˆ†ææœ€å·®çª—å£ç‰¹å¾
        window_rets = daily_returns.iloc[worst_start_idx:worst_end_idx+1]
        worst_window['stats'] = {
            'mean_daily': window_rets.mean() * 100,
            'std_daily': window_rets.std() * 100,
            'negative_days': (window_rets < 0).sum(),
            'worst_day': window_rets.min() * 100,
        }

        self.audit_results['worst_case'] = worst_window

        # ä¿å­˜å¤ç›˜åŒ…
        worst_path = os.path.join(self.output_dir, 'worst_case_window.json')
        with open(worst_path, 'w') as f:
            # è½¬æ¢æ—¥æœŸä¸ºå­—ç¬¦ä¸²
            export_data = worst_window.copy()
            export_data['start_date'] = str(export_data['start_date'])
            export_data['end_date'] = str(export_data['end_date'])
            export_data['dates'] = [str(d) for d in daily_returns.index[worst_start_idx:worst_end_idx+1].tolist()]
            json.dump(export_data, f, indent=2, cls=NumpyEncoder)

        logger.info(f"æœ€å·®çª—å£: {worst_window['start_date']} ~ {worst_window['end_date']}, "
                   f"ç´¯è®¡æ”¶ç›Š {worst_return*100:.1f}%")

        return worst_window

    def generate_report(self, strategy_name: str = 'v4') -> str:
        """
        ç”ŸæˆéªŒæ”¶æŠ¥å‘Š

        Args:
            strategy_name: ç­–ç•¥åç§°

        Returns:
            æŠ¥å‘ŠMarkdownå†…å®¹
        """
        logger.info("ç”ŸæˆéªŒæ”¶æŠ¥å‘Š...")

        report = f"""# ç”Ÿäº§éªŒæ”¶æŠ¥å‘Š - {strategy_name}

## æ‰§è¡Œä¿¡æ¯

- **å®¡è®¡æ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
- **å®¡è®¡å‘˜**: RedTeamAuditor
- **ç­–ç•¥ç‰ˆæœ¬**: {strategy_name}

---

## 1. æ•°æ®æ³„æ¼å®¡è®¡

### 1.1 asof_date æŠ½æ ·è¯æ®

"""

        asof_result = self.audit_results.get('asof_sampling', {})
        if asof_result:
            report += f"""
| æŒ‡æ ‡ | å€¼ |
|------|-----|
| æ ·æœ¬æ€»æ•° | {asof_result.get('total_samples', 'N/A')} |
| æ³„æ¼é£é™©æ•° | {asof_result.get('leakage_count', 'N/A')} |
| æ³„æ¼ç‡ | {asof_result.get('leakage_rate', 0)*100:.1f}% |
| é€šè¿‡ç‡ | {asof_result.get('pass_rate', 0)*100:.1f}% |

**ç»“è®º**: {'âœ… æ— æ³„æ¼é£é™©' if asof_result.get('leakage_count', 0) == 0 else 'âš ï¸ å­˜åœ¨æ³„æ¼é£é™©'}
"""
        else:
            report += "\n*æœªæ‰§è¡ŒasofæŠ½æ ·å®¡è®¡*\n"

        report += """
---

## 2. å¹¸å­˜è€…åå·®è¯„ä¼°

"""

        survivorship = self.audit_results.get('survivorship', {})
        if survivorship:
            report += f"""
| æŒ‡æ ‡ | å€¼ |
|------|-----|
| é£é™©ç­‰çº§ | **{survivorship.get('survivorship_risk', 'N/A')}** |
| åŸºå‡†è‚¡ç¥¨æ•° | {survivorship.get('baseline_n_stocks', 'N/A')} |

**å‹åŠ›æµ‹è¯•åœºæ™¯**:

| å‰”é™¤æ¯”ä¾‹ | éšæœºå‰”é™¤ä¼°ç®—å½±å“ | å‰”é™¤Topè´¡çŒ®ä¼°ç®—å½±å“ |
|---------|----------------|-------------------|
"""
            for scenario in survivorship.get('scenarios', []):
                ratio = scenario['drop_ratio']
                random_impact = scenario['random_drop']['estimated_impact']
                top_impact = scenario['top_contrib_drop']['estimated_impact']
                report += f"| {ratio*100:.0f}% | {random_impact} | {top_impact} |\n"

            report += f"""
**å»ºè®®**: {survivorship.get('recommendation', 'N/A')}
"""
        else:
            report += "\n*æœªæ‰§è¡Œå¹¸å­˜è€…åå·®æµ‹è¯•*\n"

        # å¹¸å­˜è€…åå·®æ¨¡å¼
        survivorship_mode = self.audit_results.get('survivorship_mode', {})
        report += f"""
### 2.1 å¹¸å­˜è€…åå·®æ¨¡å¼æ£€æŸ¥

| æ£€æŸ¥é¡¹ | ç»“æœ |
|--------|------|
| æ¨¡å¼ | {survivorship_mode.get('mode', 'N/A')} |
| é£é™©ç­‰çº§ | **{survivorship_mode.get('risk_level', 'N/A')}** |
| çŠ¶æ€ | {survivorship_mode.get('status', 'N/A')} |

**è¯´æ˜**: {survivorship_mode.get('message', 'N/A')}
"""
        if survivorship_mode.get('recommendation'):
            report += f"\n**å»ºè®®**: {survivorship_mode.get('recommendation')}\n"

        report += """
---

## 3. æˆæœ¬å‹åŠ›æµ‹è¯•

"""

        cost_stress = self.audit_results.get('cost_stress', {})
        if cost_stress:
            results = cost_stress.get('results', [])
            report += """
| å‹åŠ›ç­‰çº§ | æ¯›æ”¶ç›Š | æˆæœ¬å æ¯” | å‡€æ”¶ç›Š | æ¢æ‰‹ç‡ |
|---------|-------|---------|-------|-------|
"""
            for r in results:
                report += f"| Stress{int(r['stress_factor'])-1} | {r['gross_return']:.1f}% | " \
                         f"{r['cost_ratio']:.1f}% | {r['net_return']:.1f}% | {r['turnover']:.1f}x |\n"

            report += f"""
**P25 å‡€æ”¶ç›Š** (Stress1): {cost_stress.get('p25_return', 'N/A'):.1f}%
"""
        else:
            report += "\n*æœªæ‰§è¡Œæˆæœ¬å‹åŠ›æµ‹è¯•*\n"

        report += """
---

## 4. åˆ†å¸ƒéªŒè¯ (Walk-Forward)

"""

        wf = self.audit_results.get('walk_forward', {})
        if wf and 'error' not in wf:
            ret = wf.get('return', {})
            dd = wf.get('drawdown', {})
            report += f"""
| æŒ‡æ ‡ | P25 | P50 | P75 |
|------|-----|-----|-----|
| å¹´åŒ–æ”¶ç›Š | {ret.get('p25', 'N/A'):.1f}% | {ret.get('p50', 'N/A'):.1f}% | {ret.get('p75', 'N/A'):.1f}% |
| æœ€å¤§å›æ’¤ | {dd.get('p25', 'N/A'):.1f}% | {dd.get('p50', 'N/A'):.1f}% | {dd.get('p75', 'N/A'):.1f}% |

**æœ€å·®çª—å£**:
- æœŸé—´: {wf.get('worst_window', {}).get('period', 'N/A')}
- æ”¶ç›Š: {wf.get('worst_window', {}).get('return', 'N/A'):.1f}%
"""
        else:
            report += "\n*æœªæ‰§è¡ŒWalk-ForwardéªŒè¯*\n"

        report += """
---

## 5. çº¦æŸå½±å“è¯„ä¼°

"""

        constraint = self.audit_results.get('constraint_impact', [])
        if constraint:
            report += """
| çº¦æŸçº§åˆ« | å¹´åŒ–æ”¶ç›Š | æœ€å¤§å›æ’¤ | æ”¶ç›Šå½±å“ | å›æ’¤æ”¹å–„ |
|---------|---------|---------|---------|---------|
"""
            for c in constraint:
                report += f"| {c['constraint_level']} | {c['annual_return']:.1f}% | " \
                         f"{c['max_drawdown']:.1f}% | {c['return_impact']:+.1f}% | " \
                         f"{c['dd_improvement']:+.1f}% |\n"

            assessment = self.audit_results.get('constraint_assessment', 'N/A')
            note = self.audit_results.get('constraint_note', '')
            report += f"""
**çº¦æŸæœ‰æ•ˆæ€§è¯„ä¼°**: {assessment}

{note if note else ''}
"""
        else:
            report += "\n*æœªæ‰§è¡Œçº¦æŸå½±å“è¯„ä¼°*\n"

        # Universeå®¡è®¡
        universe = self.audit_results.get('universe', {})
        if universe and 'error' not in universe:
            report += f"""
---

## 5.1 Universeå®¡è®¡

| æŒ‡æ ‡ | å€¼ |
|------|-----|
| æŠ½æ ·æ—¥æœŸæ•° | {universe.get('n_sample_dates', 'N/A')} |
| å¹³å‡å¯äº¤æ˜“è‚¡ç¥¨æ•° | {universe.get('avg_tradable_stocks', 0):.0f} |
| æœ€å°å¯äº¤æ˜“è‚¡ç¥¨æ•° | {universe.get('min_tradable_stocks', 0):.0f} |
| æœ€å¤§å¯äº¤æ˜“è‚¡ç¥¨æ•° | {universe.get('max_tradable_stocks', 0):.0f} |
| å¹³å‡å¯äº¤æ˜“æ¯”ä¾‹ | {universe.get('avg_tradable_ratio', 0)*100:.1f}% |
| å¹³å‡ADV20 | {universe.get('avg_adv20', 0):.0f} ä¸‡å…ƒ |

**å‰”é™¤åŸå› åˆ†å¸ƒ**:
"""
            excl_summary = universe.get('exclusion_summary', {})
            for reason, count in sorted(excl_summary.items(), key=lambda x: -x[1]):
                report += f"- {reason}: {count} æ¬¡\n"

        # Lagæ•æ„Ÿæ€§å®¡è®¡
        lag_sensitivity = self.audit_results.get('lag_sensitivity', {})
        if lag_sensitivity and lag_sensitivity.get('results'):
            report += f"""
---

## 5.2 Lagæ•æ„Ÿæ€§åˆ†æ (è´¢åŠ¡å¯ç”¨æ—¥å»¶è¿Ÿ)

**è¯´æ˜**: ClickHouseæ— announce_dateå­—æ®µï¼Œä½¿ç”¨report_date + lag_daysæ¨¡æ‹Ÿè´¢åŠ¡æ•°æ®å¯ç”¨æ—¥ã€‚

| Lagå¤©æ•° | æ¨¡å¼ | å¹´åŒ–æ”¶ç›Š | æœ€å¤§å›æ’¤ | æ”¶ç›Šå·®å¼‚ |
|---------|------|---------|---------|---------|
"""
            for r in lag_sensitivity['results']:
                report += f"| {r['lag_days']} | {r['mode']} | {r['annual_return']:.2f}% | " \
                         f"{r['max_drawdown']:.1f}% | {r['return_diff_vs_base']:+.2f}% |\n"

            sensitivity = lag_sensitivity.get('sensitivity', {})
            report += f"""
**æ•æ„Ÿæ€§æŒ‡æ ‡**:
- æ”¶ç›Šå˜åŠ¨èŒƒå›´: {sensitivity.get('range', 'N/A'):.2f}%
- æœ€ä¼˜Lag: {sensitivity.get('best_lag', 'N/A')} å¤©
- æœ€å·®Lag: {sensitivity.get('worst_lag', 'N/A')} å¤©

**å»ºè®®**: {lag_sensitivity.get('recommendation', 'å»ºè®®ä½¿ç”¨paperæ¨¡å¼(lag=60å¤©)è¿›è¡Œå›æµ‹')}
"""

        # æœ€ç»ˆç»“è®º
        report += """
---

## 6. æœ€ç»ˆç»“è®º

"""

        # åˆ¤æ–­GO/NO-GO
        go_conditions = []

        # æ£€æŸ¥å„é¡¹æ¡ä»¶
        if asof_result.get('leakage_count', 1) == 0:
            go_conditions.append(('asofæ— æ³„æ¼', True))
        else:
            go_conditions.append(('asofæ— æ³„æ¼', False))

        # å¹¸å­˜è€…åå·®æ¨¡å¼æ£€æŸ¥
        survivorship_mode = self.audit_results.get('survivorship_mode', {})
        if survivorship_mode.get('status') == 'PASS':
            go_conditions.append(('å¹¸å­˜è€…åå·®: åŠ¨æ€Universe', True))
        else:
            go_conditions.append(('å¹¸å­˜è€…åå·®: åŠ¨æ€Universe', False))

        if cost_stress.get('p25_return', 0) >= 18:
            go_conditions.append(('Stress1 P25â‰¥18%', True))
        else:
            go_conditions.append(('Stress1 P25â‰¥18%', False))

        all_passed = all(c[1] for c in go_conditions)

        report += "### éªŒæ”¶æ¡ä»¶\n\n"
        for condition, passed in go_conditions:
            status = "âœ…" if passed else "âŒ"
            report += f"- {status} {condition}\n"

        report += f"""
### ç»“è®º

**{'ğŸŸ¢ GO - å…è®¸è¿›å…¥ Paper Trading' if all_passed else 'ğŸ”´ NO-GO - éœ€è¦å›é€€åˆ° v3 æˆ–ä¿®å¤é—®é¢˜'}**

"""

        if not all_passed:
            report += """### ä¿®å¤å»ºè®®

1. ä¿®å¤æ•°æ®æ³„æ¼é—®é¢˜
2. é‡æ–°è¯„ä¼°æˆæœ¬æ¨¡å‹
3. è€ƒè™‘é™ä½æ¢æ‰‹ç‡
4. æ£€æŸ¥çº¦æŸå®ç°
"""

        report += """
---

## 7. å¾…ç¡®è®¤å‚æ•° (Checklist)

- [ ] ClickHouse ä¸­æ˜¯å¦æœ‰ `announce_date` å­—æ®µï¼Ÿ
- [ ] é€€å¸‚è‚¡ç¥¨æ•°æ®æ˜¯å¦å¯ç”¨ï¼Ÿ
- [ ] å®é™…ä½£é‡‘è´¹ç‡ç¡®è®¤
- [ ] é¢„æœŸèµ„é‡‘è§„æ¨¡ç¡®è®¤
- [ ] è¡Œä¸šåˆ†ç±»æ•°æ®ç¡®è®¤

---

## 8. è¯æ®æ–‡ä»¶

- `asof_samples.csv` - asofæŠ½æ ·è¯æ®
- `cost_stress.csv` - æˆæœ¬å‹åŠ›æµ‹è¯•ç»“æœ
- `constraint_impact.csv` - çº¦æŸå½±å“è¯„ä¼°
- `worst_case_window.json` - æœ€å·®çª—å£å¤ç›˜
- `universe_audit_stats.csv` - Universeå®¡è®¡ç»Ÿè®¡
- `universe_exclusion_reasons.csv` - Universeå‰”é™¤åŸå› 
- `lag_sensitivity.csv` - Lagæ•æ„Ÿæ€§åˆ†æ

---

*æŠ¥å‘Šç”Ÿæˆæ—¶é—´: {timestamp}*
""".format(timestamp=datetime.now().isoformat())

        # ä¿å­˜æŠ¥å‘Š
        report_path = os.path.join(self.output_dir, 'prod_acceptance_report.md')
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(report)

        logger.info(f"éªŒæ”¶æŠ¥å‘Šå·²ä¿å­˜: {report_path}")

        return report

    def save_all_results(self, run_id: str):
        """ä¿å­˜æ‰€æœ‰ç»“æœåˆ°æ ‡å‡†åŒ–ç›®å½•"""
        import shutil

        # åˆ›å»ºè¿è¡Œç›®å½•
        run_dir = os.path.join(os.path.dirname(self.output_dir), run_id)
        os.makedirs(run_dir, exist_ok=True)

        # å¤åˆ¶è¯æ®æ–‡ä»¶
        redteam_dir = os.path.join(run_dir, 'redteam_samples')
        os.makedirs(redteam_dir, exist_ok=True)

        for filename in os.listdir(self.output_dir):
            src = os.path.join(self.output_dir, filename)
            dst = os.path.join(redteam_dir, filename)
            if os.path.isfile(src):
                shutil.copy(src, dst)

        # ä¿å­˜å®¡è®¡ç»“æœ
        results_path = os.path.join(run_dir, 'metrics.json')
        with open(results_path, 'w') as f:
            json.dump(self.audit_results, f, indent=2, cls=NumpyEncoder)

        logger.info(f"æ‰€æœ‰ç»“æœå·²ä¿å­˜åˆ°: {run_dir}")

        return run_dir
